---
title: "Home Prices EDA and Machine Learning"
output: html_document

---

```{r setup, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Add Libraries
```{r, include=FALSE}
library(data.table)
library(ggplot2)
library(ggthemes)
library(glmnet)
library(plotly)
library(MASS)
library(caret)
library(leaps)
library(tibble)
library(magrittr) 
library(dplyr)
library(tidyverse)
library(scales)
library(tree)
library(randomForest)
library(rpart)
library(rpart.plot)
theme_set(theme_bw())
```

#### Load in dataset
```{r}
#install.packages('bit64')
Price <- fread("trainPrice.csv")
#Price <- fread("/Users/paulademacker/Documents/USA/Boston U/2. term/BA810/Team Project/trainPrice.csv")
```

#### First few rows
We can see that we have a total of 25 features in the dataset
```{r}
head(Price, 5)
```

#### Structure of the dataset
There are a total of 1,601,458 observations
```{r}
str(Price)
transform(Price, transaction_real_price = as.numeric(transaction_real_price))
#summary(Price)
```


### Cleaning the Data

#### view the count of null values in each column
The feature, total_parking_capacity_in_site has the largest number of null values (91813)
```{r}
Price[, lapply(.SD, function(x) sum(is.na(x)))]
```
#### View all unqiue values present in columns
```{r}
unique(Price$heat_type)
```
##### How many are empty strings?
```{r}
Price[heat_type == ''][, .N]
```
##### We can remove these empty string values to create dummy variables later
```{r}
Price <- Price[heat_type != '']
```


```{r}
Price[heat_fuel == ''][, .N]
```


```{r}
Price[heat_fuel == '-'][, .N]
```
##### We can remove these empty string values to create dummy variables later
```{r}
Price <- Price[heat_fuel != '']
```

##### We can remove these empty string values to create dummy variables later
```{r}
Price <- Price[heat_fuel != '-']
```

```{r}
unique(Price$bathroom_count)
```
```{r}
unique(Price$city)
```
```{r}
unique(Price$room_count)
```
```{r}
unique(Price$front_door_structure)
```
```{r}
Price[front_door_structure == '-'][, .N]
```


##### count of values that are empty strings
```{r}
Price[front_door_structure == ''][, .N]
```
##### We can remove these empty string values to create dummy variables later
```{r}
Price <- Price[front_door_structure != '']
```
##### We can remove these empty dash values to create dummy variables later
```{r}
Price <- Price[front_door_structure != '-']
```

```{r}
unique(Price$year_of_completion)
```
#### Drop rows room_count as 8
```{r}
Price <- Price[room_count != 8]
```

#### Remove all rows that have missing values
There are a total of 1601458 million observations, and we will be removing less than 100,000 of them.
We can confirm that rows were removed using .N (the number of observations)
```{r}
Price <- na.omit(Price)
Price[, .N]
```

#### Set city and room count as factor
```{r}
# Price <- Price[, city:=as.factor(city)]
# Price <- Price[, room_count:=as.factor(room_count)]
# Price <- Price[, bathroom_count:=as.factor(bathroom_count)]
#Price <- Price[, transaction_year_month := as.POSIXct(transaction_year_month, format='%Y%m')]
#Price <- Price[, year_of_completion := as.Date(year_of_completion, format='%Y')]

str(Price)
```


##### We can create dummy variables and convert them to factors
```{r}
# Price <- fastDummies::dummy_cols(Price, select_columns = c("heat_fuel","heat_type","front_door_structure"))
# 
# Price <- Price[, heat_fuel_cogeneration:=as.factor(heat_fuel_cogeneration)]
# Price <- Price[, heat_fuel_gas :=as.factor(heat_fuel_gas)]
# Price <- Price[, heat_type_central:=as.factor(heat_type_central)]
# Price <- Price[, heat_type_district:=as.factor(heat_type_district)]
# Price <- Price[, heat_type_individual:=as.factor(heat_type_individual)]
# Price <- Price[, front_door_structure_corridor:=as.factor(front_door_structure_corridor)]
# Price <- Price[, front_door_structure_mixed:=as.factor(front_door_structure_mixed)]
# Price <- Price[, front_door_structure_stairway:=as.factor(front_door_structure_stairway)]
Price[,transaction_real_price := as.numeric(transaction_real_price)] 
Price[,address_by_law := as.numeric(address_by_law)]
str(Price)
```

```{r}
unique(Price$transaction_date)
unique(Price$heat_type)
unique(Price$heat_fuel)
unique(Price$front_door_structure)

#I found transaction_date,heat_type, heat_fuel, front_door_structure didn't become dummy till here (Sylar)
Price <- Price[transaction_date == "1~10", transaction_date:=0]
Price <- Price[transaction_date == "11~20", transaction_date:=1]
Price <- Price[transaction_date == "21~30", transaction_date:=2]
Price <- Price[transaction_date == "21~28", transaction_date:=2]
Price <- Price[transaction_date == "21~29", transaction_date:=2]
Price <- Price[transaction_date == "21~31", transaction_date:=2]
Price[,transaction_date := as.numeric(transaction_date)]

Price <- Price[heat_type == "individual", heat_type:=0]
Price <- Price[heat_type == "central", heat_type:=1]
Price <- Price[heat_type == "district", heat_type:=2]
Price[,heat_type := as.numeric(heat_type)]

Price <- Price[heat_fuel == "gas", heat_fuel:=0]
Price <- Price[heat_fuel == "cogeneration", heat_fuel:=1]
Price[,heat_fuel := as.numeric(heat_fuel)]

Price <- Price[front_door_structure == "corridor", front_door_structure:=0]
Price <- Price[front_door_structure == "stairway", front_door_structure:=1]
Price <- Price[front_door_structure == "mixed", front_door_structure:=2]
Price[,front_door_structure := as.numeric(front_door_structure)]

str(Price)
```













#### Barchart showing real price and city
```{r}
ggplot(Price, aes(x=as.factor(city), y = transaction_real_price)) + 
  geom_bar(stat = "summary", fun = "mean")+
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
  theme_classic()
```

#### Barchart showing real price and room count
```{r}
ggplot(Price, aes(x=room_count, y = transaction_real_price)) + 
  geom_bar(stat = "summary", fun = "mean")+
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
  theme_classic()
```
#### Barchart showing real price and front_door_structure
```{r}
ggplot(Price, aes(x=front_door_structure, y = transaction_real_price)) + 
  geom_bar(stat = "summary", fun = "mean")+
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
  theme_classic()
```

#### Barchart showing real price and heat_fuel
```{r}
ggplot(Price, aes(x=heat_fuel, y = transaction_real_price)) + 
  geom_bar(stat = "summary", fun = "mean")+
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
  theme_classic()
```
#### Barchart showing real price and heat_type
```{r}
ggplot(Price, aes(x=heat_type, y = transaction_real_price)) + 
  geom_bar(stat = "summary", fun = "mean")+
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
  theme_classic()
```





####Plotly Map showing locations of apartments
Still working on getting colors to appear for prices
```{r}
# fig <- head(Price, 100000)
# fig <- fig %>%
#   plot_ly(
#     lat = ~latitude,
#     lon = ~longitude,
#     #marker = list(color = "fuchsia"),
#     color = Price[,"transaction_real_price"],
#     type = 'scattermapbox',
#     hovertext = Price[,"transaction_real_price"]) 
# fig <- fig %>%
#   layout(
#     mapbox = list(
#       style = 'open-street-map',
#       zoom =2.5,
#       center = list(longitude = 35.963911, latitude = 127.919770))) 
# 
# fig
```










## Linear Regression
#### Randomize the rows
```{r}
set.seed(42)
rows <- sample(nrow(Price))
Price <- Price[rows, ]
```


#### Linear model of price and city (the first 50000 observations)
```{r}
# ggplot(head(Price, 50000), aes(x = city
# , y=transaction_real_price)) +
#   geom_point(alpha = 0.4)+
#   stat_smooth(
#     method = "lm",
#     color = "#C42126",
#     se = FALSE,
#     size = 1
# )
```



#### Linear regression model of exclusive use area vs real price
```{r}
# ggplot(Price, aes(x = exclusive_use_area, y=transaction_real_price)) +
#   geom_point(alpha = 0.4)
```









# Linear Regression
```{r}
Price[, transaction_real_price := as.numeric(transaction_real_price)]
# we want to select random columns adn set the seed to get consistant results
set.seed(810)

# select random number
rnorm(1)



# Split tha data
# assign 100K random rows to the test set
# test index is a random set of 320000 numbers
# this way we can do an 80/20 split
test_index <- sample(nrow(Price), 320000)
# now split
# we use that set of random numbers to select those random rows
dd_test <- Price[test_index,]
dd_train <- Price[-test_index,]


# our response variables to use later
y_train <- dd_train$transaction_real_price
y_test <- dd_test$transaction_real_price



# create our predictors

# throwing some variables that seem to make sense to give it a try

f1 <- as.formula(transaction_real_price ~ year_of_completion + exclusive_use_area +floor + total_parking_capacity_in_site+ total_household_count_in_sites +apartment_building_count_in_sites +heat_type + heat_fuel +supply_area + total_household_count_of_area_type +room_count +bathroom_count+front_door_structure)

# matrix with x values
x1_train_sample <- model.matrix(f1, dd_train)
x1_test <- model.matrix(f1, dd_test)

# get columns (for forward regression later)
#for (col in names(Price))
#{
 
 # print(col) 
  
#}


fit_lm1 <- lm(f1, dd_train)


yhat_train_lm1 <- predict(fit_lm1)
mse_train_lm1 <- mean((y_train - yhat_train_lm1)^2)
paste("Linear Regression Train MSE",mse_train_lm1)

yhat_test_lm1 <- predict(fit_lm1, dd_test)
mse_test_lm1 <- mean((y_test - yhat_test_lm1)**2)
paste("Linear Regression Test MSE",mse_test_lm1)


str(dd_train)


(y_test[0] - yhat_test_lm1[0])


class(Price$transaction_real_price)
```

```{r}
summary(Price)
```



#### Forward Selection
```{r}
xnames <- colnames(dd_train)
xnames <- xnames[!xnames %in% c("key","apartment_id","transaction_year_month","latitude","longitude","address_by_law","room_id")]
fit_fw <- lm(transaction_real_price ~ 1, data = dd_train)
yhat_train <- predict(fit_fw, dd_train)
yhat_test<- predict(fit_fw, dd_test)
mse_train <- mean((dd_train$transaction_real_price - yhat_train)^2)
mse_test<-mean((dd_test$transaction_real_price - yhat_test)^2)
xname <- "intercept"

log_fw <-
  tibble(
    xname = xname,
    model = paste0(deparse(fit_fw), collapse = ""),
    mse_train = mse_train,
    mse_test = mse_test
  )
###
while (length(xnames) > 0) {
  best_mse_train <- NA
  best_mse_test <- NA
  best_fit_fw <- NA
  best_xname <- NA
  # select the next best predictor
  for (xname in xnames) {
    # take a moment to examine and understand the following line
    fit_fw_tmp <- update(fit_fw, as.formula(paste0(". ~ . + ", xname)))
    #compute MSE train
    yhat_train_tmp <- predict(fit_fw_tmp, dd_train)
    mse_train_tmp <- mean((dd_train$transaction_real_price - yhat_train_tmp) ^ 2)
    # compute MSE test
    yhat_test_tmp <- predict(fit_fw_tmp, dd_test)
    mse_test_tmp <- mean((dd_test$transaction_real_price - yhat_test_tmp) ^ 2)
    # if this is the first predictor to be examined,
    # or if this predictors yields a lower MSE that the current
    # best, then store this predictor as the current best predictor
    if (is.na(best_mse_test) | mse_test_tmp < best_mse_test) {
      best_xname <- xname
      best_fit_fw <- fit_fw_tmp
      best_mse_train <- mse_train_tmp
      best_mse_test <- mse_test_tmp
    }
  }
  log_fw <-
    log_fw %>% add_row(
      xname = best_xname,
      model = paste0(deparse(best_fit_fw$call), collapse = ""),
      mse_train = best_mse_train,
      mse_test = best_mse_test
    )
  # adopt the best model for the next iteration
  fit_fw <- best_fit_fw
  # remove the current best predictor from the list of predictors
  xnames <- xnames[xnames!=best_xname]
}
ggplot(log_fw, aes(seq_along(xname), mse_test)) +
  geom_point() +
  geom_line() +
  geom_point(aes(y=mse_train), color="blue") +
  geom_line(aes(y=mse_train), color="blue") +
  scale_x_continuous("Variables",labels = log_fw$xname,breaks = seq_along(log_fw$xname)) +
  scale_y_continuous("MSE test") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


```{r}
print(log_fw)
```
#### The way above has some diffculity showing equations/equations,so I tried another way

```{r}

# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.model <- train(transaction_real_price ~., data = dd_train,
                    method = "leapForward", 
                    tuneGrid = data.frame(nvmax = 1:24),
                    trControl = train.control
                    )
step.model$results
```
```{r}
str(min(step.model$results$RMSE
    ))
```


```{r}
step.model$bestTune
summary(step.model$finalModel)
```

#### Backward selection
```{r}

# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.model <- train(transaction_real_price ~., data = dd_train,
                    method = "leapBackward", 
                    tuneGrid = data.frame(nvmax = 1:24),
                    trControl = train.control
                    )
step.model$results
```


```{r}
step.model$bestTune
summary(step.model$finalModel)
```

#### This is another package doing forward/backward, however, having error

```{r}
#library(olsrr)
#model <- lm(y_train ~ ., data = dd_train)
#print(model)

#ols_step_forward_p(model)
```


```{r}
#model <- lm(transaction_real_price ~ ., data = dd_train)
#k <- ols_step_forward_p(model)
#plot(k)
```


```{r}
# stepwise forward regression
#model <- lm(transaction_real_price ~ ., data = dd_train)
#ols_step_forward_p(model, details = TRUE)
```


#### Sylar's part done till here, Please feel free to build upon this version

#### Ridge Regression
```{r}
fit.ridge <- cv.glmnet(x1_train_sample, y_train, alpha = 0)

predict(fit.ridge,
        type = "coefficients",
        s = fit.ridge$lambda)

to_plot <- data.table(
  lambda = fit.ridge$lambda,
  coef_value = ridge.coef[2, ]
)
ggplot(to_plot, aes(log(lambda), coef_value)) +
  geom_line() +
  theme_few()

yhat.train.ridge <- predict(fit.ridge, x1_train_sample, s = fit.ridge$lambda.min)
mse.train.ridge <- mean((y_train - yhat.train.ridge)^2)

yhat.test.ridge <- predict(fit.ridge, x1_test, s = fit.ridge$lambda.min)
mse.test.ridge <- mean((y_test - yhat.test.ridge)^2)

cat("Train MSE: ",mse.train.ridge)
cat(" Test MSE: ",mse.test.ridge)
#Test MSE is minimal lower than Train MSE
```

#### Lasso Regression
```{r}
fit.lasso <- cv.glmnet(x1_train_sample, y_train, alpha = 1)

yhat.train.lasso <- predict(fit.lasso, x1_train_sample, s = fit.lasso$lambda.min)
mse.train.lasso <- mean((y_train - yhat.train.lasso)^2)

yhat.test.lasso <- predict(fit.lasso, x1_test, s = fit.lasso$lambda.min)
mse.test.lasso <- mean((y_test - yhat.test.lasso)^2)

cat("Train MSE: ",mse.train.lasso)
cat(" Test MSE: ",mse.test.lasso)
#Test MSE is slightly higher than the Train MSE but better than the Ridge Regression

```


#### Decision Tree
```{r}
set.seed(217)
train = sample(1:nrow(Price),nrow(Price)*3/4)
tree.testy = Price[-train,transaction_real_price]
tree.test = Price[-train]

tree.price = tree(transaction_real_price ~ .,Price,subset = train)
summary(tree.price)
```
```{r}
#plot(tree.price)
#text(tree.price,pretty = 0)
# Not exactly nice graph
# Prune Tree
cv.price = cv.tree(tree.price)
plot(cv.price$size,cv.price$dev, type = "b")
cv.price
# size = 14 have the lowest cv error
# which seems like there is no much to prune the decision tree

```

```{r}
#Test MSE
tree.yhat = predict(tree.price,newdata=tree.test)
mean((tree.yhat - tree.testy)^2) 
``` 


```{r}
#Lecture Method to deal with decision tree
set.seed(217)
tree.price.lec <- rpart(transaction_real_price ~ ., Price,subset = train,
                    control = rpart.control(cp=0.01))
rpart.plot(tree.price.lec,type = 1)

plotcp(tree.price)

```




#### Bagging
```{r}
library(randomForest)
set.seed(217)
test.mse = c()
for (i in seq(1,5)) {
train = sample(1:nrow(Price),(nrow(Price)/100)*i)
tree.testy = Price[-train,transaction_real_price]
tree.test = Price[-train]
bag.price = randomForest(transaction_real_price ~ ., data = Price, subset = train, mtry = 24, importance = TRUE)
yhat.bag = predict(bag.price, newdata = tree.test)
test.mse = c(test.mse,mean((tree.testy-yhat.bag)^2))
}
test.mse 

```

```{r}
plot(x=seq(1,5),test.mse)
```

#### Random Forest
```{r}

train = sample(1:nrow(Price),(nrow(Price)/100))
tree.testy = Price[-train,transaction_real_price]
tree.test = Price[-train]
rf.price = randomForest(transaction_real_price ~ ., data = Price, subset = train, mtry = 8, importance = TRUE)
yhat.rf = predict(rf.price, newdata = tree.test)
mean((tree.testy-yhat.rf)^2)a
```

```{r}
importance(rf.price)
```

```{r}
varImpPlot(rf.price)
```

#### Boosting
```{r}
library(gbm)
set.seed(217)
train_sample = sample(1:nrow(Price),(nrow(Price)/10))
tree.testy_sample = Price[-train_sample,transaction_real_price]
boost.price = gbm(transaction_real_price ~ ., data = Price[train_sample,], distribution = "gaussian", n.trees = 5000, interaction.depth = 4 )
summary(boost.price)
```

```{r}
#Evaluate boosted tree model
yhat.boost = predict(boost.price, newdata = Price[-train_sample,], n.trees = 5000)
mean((tree.testy_sample - yhat.boost)^2)
```




```{r}
set.seed(217)
tree.price <- rpart(transaction_real_price ~ ., Price,subset = train,
                    control = rpart.control(cp=0.01))
rpart.plot(tree.price,type = 1)
```



```{r}
plotcp(tree.price)
```

```{r}
#Prune tree
ptree = prune(tree.price,cp = tree.price$cptable[which.min(tree.price$cptable[,"xerror"]),"CP"])
rpart.plot(ptree,type = 1)
```




